{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nimport numpy as np\nimport os\nimport time\n\npath_to_file = tf.keras.utils.get_file('shakespeare.txt',\n                                       'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n\n# Read, then decode for py2 compat.\ntext = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n# length of text is the number of characters in it\nprint('Length of text: {} characters'.format(len(text)))\n\nprint(text[:250])\n\nvocab = sorted(set(text))\n\nchar2idx = {u: i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ntext_as_int = np.array([char2idx[c] for c in text])\nprint(text_as_int)\n# The maximum length sentence we want for a single input in characters\nseq_length = 100\nexamples_per_epoch = len(text) // (seq_length + 1)\n\n# Create training examples / targets\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n\nfor i in char_dataset.take(5):\n    print(idx2char[i.numpy()])\n\nsequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n\nfor item in sequences.take(5):\n    print(repr(''.join(idx2char[item.numpy()])))\n\n\ndef split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\n\ndataset = sequences.map(split_input_target)\n\nfor input_example, target_example in dataset.take(1):\n    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n\nfor i, (input_idx, target_idx) in enumerate(zip(input_example[:10], target_example[:10])):\n    print(\"Step {:4d}\".format(i))\n    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n\n\n\n\n# Batch size\nBATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\n# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units\nrnn_units = 1024\n\ndef build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.GRU(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n  return model\n\nmodel = build_model(\n    vocab_size = len(vocab),\n    embedding_dim=embedding_dim,\n    rnn_units=rnn_units,\n    batch_size=BATCH_SIZE)\n\nfor input_example_batch, target_example_batch in dataset.take(1):\n  example_batch_predictions = model(input_example_batch)\n  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n\nprint(model.summary)\n\ndef loss(labels, logits):\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nexample_batch_loss  = loss(target_example_batch, example_batch_predictions)\n\n\nmodel.compile(optimizer='adam', loss=loss)\n\n# Directory where the checkpoints will be saved\ncheckpoint_dir = './training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)\n\nEPOCHS=50\n\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","execution_count":7,"outputs":[{"output_type":"stream","text":"Length of text: 1115394 characters\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\n[18 47 56 ... 45  8  0]\nF\ni\nr\ns\nt\n'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\nInput data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\nTarget data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\nStep    0\n  input: 18 ('F')\n  expected output: 47 ('i')\nStep    1\n  input: 47 ('i')\n  expected output: 56 ('r')\nStep    2\n  input: 56 ('r')\n  expected output: 57 ('s')\nStep    3\n  input: 57 ('s')\n  expected output: 58 ('t')\nStep    4\n  input: 58 ('t')\n  expected output: 1 (' ')\nStep    5\n  input: 1 (' ')\n  expected output: 15 ('C')\nStep    6\n  input: 15 ('C')\n  expected output: 47 ('i')\nStep    7\n  input: 47 ('i')\n  expected output: 58 ('t')\nStep    8\n  input: 58 ('t')\n  expected output: 47 ('i')\nStep    9\n  input: 47 ('i')\n  expected output: 64 ('z')\n(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n<bound method Model.summary of <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f1e1024dcd0>>\nEpoch 1/50\n172/172 [==============================] - 7s 43ms/step - loss: 2.6723\nEpoch 2/50\n172/172 [==============================] - 7s 42ms/step - loss: 1.9710\nEpoch 3/50\n172/172 [==============================] - 7s 43ms/step - loss: 1.7001\nEpoch 4/50\n172/172 [==============================] - 7s 43ms/step - loss: 1.5491\nEpoch 5/50\n172/172 [==============================] - 7s 43ms/step - loss: 1.4577\nEpoch 6/50\n172/172 [==============================] - 7s 42ms/step - loss: 1.3982\nEpoch 7/50\n172/172 [==============================] - 7s 42ms/step - loss: 1.3522\nEpoch 8/50\n172/172 [==============================] - 7s 42ms/step - loss: 1.3131\nEpoch 9/50\n172/172 [==============================] - 7s 43ms/step - loss: 1.2791\nEpoch 10/50\n172/172 [==============================] - 7s 42ms/step - loss: 1.2469\nEpoch 11/50\n172/172 [==============================] - 8s 44ms/step - loss: 1.2148\nEpoch 12/50\n172/172 [==============================] - 7s 42ms/step - loss: 1.1828\nEpoch 13/50\n172/172 [==============================] - 7s 43ms/step - loss: 1.1503\nEpoch 14/50\n172/172 [==============================] - 7s 43ms/step - loss: 1.1184\nEpoch 15/50\n172/172 [==============================] - 7s 42ms/step - loss: 1.0830\nEpoch 16/50\n172/172 [==============================] - 7s 42ms/step - loss: 1.0485\nEpoch 17/50\n172/172 [==============================] - 7s 42ms/step - loss: 1.0122\nEpoch 18/50\n172/172 [==============================] - 8s 46ms/step - loss: 0.9780\nEpoch 19/50\n172/172 [==============================] - 7s 43ms/step - loss: 0.9424\nEpoch 20/50\n172/172 [==============================] - 7s 42ms/step - loss: 0.9080\nEpoch 21/50\n172/172 [==============================] - 7s 43ms/step - loss: 0.8736\nEpoch 22/50\n172/172 [==============================] - 7s 42ms/step - loss: 0.8443\nEpoch 23/50\n172/172 [==============================] - 7s 43ms/step - loss: 0.8158\nEpoch 24/50\n172/172 [==============================] - 7s 43ms/step - loss: 0.7918\nEpoch 25/50\n172/172 [==============================] - 7s 44ms/step - loss: 0.7697\nEpoch 26/50\n172/172 [==============================] - 7s 42ms/step - loss: 0.7498\nEpoch 27/50\n172/172 [==============================] - 7s 42ms/step - loss: 0.7328\nEpoch 28/50\n172/172 [==============================] - 7s 42ms/step - loss: 0.7185\nEpoch 29/50\n172/172 [==============================] - 7s 42ms/step - loss: 0.7046\nEpoch 30/50\n172/172 [==============================] - 7s 42ms/step - loss: 0.6928\nEpoch 31/50\n172/172 [==============================] - 7s 43ms/step - loss: 0.6845\nEpoch 32/50\n172/172 [==============================] - 7s 43ms/step - loss: 0.6747\nEpoch 33/50\n172/172 [==============================] - 7s 43ms/step - loss: 0.6674\nEpoch 34/50\n172/172 [==============================] - 7s 43ms/step - loss: 0.6598\nEpoch 35/50\n172/172 [==============================] - 7s 43ms/step - loss: 0.6557\nEpoch 36/50\n172/172 [==============================] - 7s 42ms/step - loss: 0.6484\nEpoch 37/50\n172/172 [==============================] - 7s 42ms/step - loss: 0.6441\nEpoch 38/50\n172/172 [==============================] - 7s 44ms/step - loss: 0.6402\nEpoch 39/50\n172/172 [==============================] - 8s 44ms/step - loss: 0.6376\nEpoch 40/50\n172/172 [==============================] - 7s 43ms/step - loss: 0.6330\nEpoch 41/50\n172/172 [==============================] - 7s 43ms/step - loss: 0.6331\nEpoch 42/50\n172/172 [==============================] - 7s 42ms/step - loss: 0.6304\nEpoch 43/50\n172/172 [==============================] - 8s 44ms/step - loss: 0.6279\nEpoch 44/50\n172/172 [==============================] - 7s 43ms/step - loss: 0.6275\nEpoch 45/50\n172/172 [==============================] - 7s 43ms/step - loss: 0.6265\nEpoch 46/50\n172/172 [==============================] - 7s 43ms/step - loss: 0.6238\nEpoch 47/50\n172/172 [==============================] - 7s 42ms/step - loss: 0.6263\nEpoch 48/50\n172/172 [==============================] - 7s 43ms/step - loss: 0.6269\nEpoch 49/50\n172/172 [==============================] - 7s 42ms/step - loss: 0.6231\nEpoch 50/50\n172/172 [==============================] - 7s 43ms/step - loss: 0.6206\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"tf.train.latest_checkpoint(checkpoint_dir)\n\nmodel = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))\n\nmodel.summary()","execution_count":8,"outputs":[{"output_type":"stream","text":"Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (1, None, 256)            16640     \n_________________________________________________________________\ngru_3 (GRU)                  (1, None, 1024)           3938304   \n_________________________________________________________________\ndense_3 (Dense)              (1, None, 65)             66625     \n=================================================================\nTotal params: 4,021,569\nTrainable params: 4,021,569\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_text(model, start_string):\n  # Evaluation step (generating text using the learned model)\n\n  # Number of characters to generate\n  num_generate = 1000\n\n  # Converting our start string to numbers (vectorizing)\n  input_eval = [char2idx[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n\n  # Empty string to store our results\n  text_generated = []\n\n  # Low temperatures results in more predictable text.\n  # Higher temperatures results in more surprising text.\n  # Experiment to find the best setting.\n  temperature = 0.3\n\n  # Here batch size == 1\n  model.reset_states()\n  for i in range(num_generate):\n    predictions = model(input_eval)\n    # remove the batch dimension\n    predictions = tf.squeeze(predictions, 0)\n\n    # using a categorical distribution to predict the character returned by the model\n    predictions = predictions / temperature\n    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n    # We pass the predicted character as the next input to the model\n    # along with the previous hidden state\n    input_eval = tf.expand_dims([predicted_id], 0)\n\n    text_generated.append(idx2char[predicted_id])\n\n  return (start_string + ''.join(text_generated))","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(generate_text(model, start_string=u\"GORKEM: \"))","execution_count":20,"outputs":[{"output_type":"stream","text":"EGEMEN: which is the wedding-day.\n\nKING RICHARD III:\nMadam, I hope I may not be satisfied!\n\nKING RICHARD III:\nThe one hither to go speed.\n\nSICINIUS:\nThe gods have some way say he might encounter'd high to answer;\nCome she is sweetent down and husband,\nAnd bow discourance and obeying him.\nThou dost say 'twas by prisoner's death,\nAnd he that sent upon thy father's life.\nWhat's he that should suck thee all at once,\nFor what they are here to chide the rest.\n\nKING RICHARD III:\nHe said the truth; but the king straight shall say.\n\nKeeper:\nMy lord, I have it for our vantage.\n\nPETRUCHIO:\nI say it is the moon.\n\nKATHARINA:\nThere's some in hope is sweetent in a cause;\nThe people do admit you to your walls;\nRichard Lucentio is coming hither: but at hand, at hand,\nThat if our quarrel to the worst be in the common means\nTo prove it true to love, and I will serve him\nWhere he abroad and that with their lives before:\nThe storm of Marcius' hands again.\nJuliet, for lesser's hand, and go before, no house,\nAnd aft\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}